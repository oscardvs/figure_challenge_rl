#!/usr/bin/env python3
"""Add synthetic chain-of-thought reasoning to expert trajectories via API.

Takes expert trajectories (from collect_expert_trajectories.py) and annotates
each (observation, action) pair with 2-4 sentences of reasoning generated by
a cheap LLM (Gemini Flash by default). The reasoning is inserted as the
"reasoning" field in each step, which sft_train.py already handles via
format_as_conversations() to produce <think>reasoning</think>action format.

Usage:
    python -m src.training.generate_cot
    python -m src.training.generate_cot --input-dir data/expert_trajectories --provider google
    python -m src.training.generate_cot --provider anthropic --model claude-sonnet-4-20250514
"""

from __future__ import annotations

import argparse
import hashlib
import json
import logging
import sys
import time
from pathlib import Path

import yaml
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
load_dotenv(Path(__file__).resolve().parents[2] / ".env")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[2]

COT_PROMPT = """\
You are annotating expert browser agent trajectories with chain-of-thought reasoning.

Given the current page observation and the expert action taken, write 2-4 sentences
explaining WHY this action was chosen. Focus on:
1. What clues in the observation led to this action.
2. What the action is expected to accomplish.
3. Any puzzle-solving logic (e.g., "the code appears to be hidden in shadow DOM").

Be concise and specific. Do NOT repeat the observation or action verbatim.

## Current Observation (truncated):
{obs_text}

## Expert Action:
{action}

## Reasoning (2-4 sentences):"""


def _get_client(provider: str, model: str):
    """Create an API client for the given provider."""
    if provider == "google":
        from google import genai
        return genai.Client(), model
    elif provider == "anthropic":
        import anthropic
        return anthropic.Anthropic(), model
    elif provider == "openai":
        import openai
        return openai.OpenAI(), model
    else:
        raise ValueError(f"Unknown provider: {provider}")


def _call_api(client, model: str, provider: str, prompt: str) -> str:
    """Make a single API call to generate reasoning."""
    if provider == "google":
        response = client.models.generate_content(
            model=model,
            contents=prompt,
        )
        return response.text.strip()
    elif provider == "anthropic":
        response = client.messages.create(
            model=model,
            max_tokens=256,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.content[0].text.strip()
    elif provider == "openai":
        response = client.chat.completions.create(
            model=model,
            max_tokens=256,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content.strip()
    return ""


def generate_reasoning(
    obs_text: str,
    action: str,
    client,
    model: str,
    provider: str,
    cache: dict[str, str],
) -> str:
    """Generate reasoning for a single (observation, action) pair.

    Uses a content-hash cache to avoid redundant API calls.
    """
    # Truncate observation to keep API cost low.
    obs_truncated = obs_text[:3000] if len(obs_text) > 3000 else obs_text

    cache_key = hashlib.md5(
        f"{obs_truncated[:500]}|{action}".encode()
    ).hexdigest()

    if cache_key in cache:
        return cache[cache_key]

    prompt = COT_PROMPT.format(obs_text=obs_truncated, action=action)

    try:
        reasoning = _call_api(client, model, provider, prompt)
    except Exception as e:
        logger.warning(f"API call failed: {e}")
        reasoning = ""

    cache[cache_key] = reasoning
    return reasoning


def annotate_trajectory(
    trajectory: dict,
    client,
    model: str,
    provider: str,
    cache: dict[str, str],
) -> dict:
    """Add reasoning field to each step in the trajectory."""
    for step in trajectory.get("steps", []):
        if step.get("reasoning"):
            continue  # Already annotated.

        reasoning = generate_reasoning(
            obs_text=step.get("obs_text", ""),
            action=step.get("action", ""),
            client=client,
            model=model,
            provider=provider,
            cache=cache,
        )
        step["reasoning"] = reasoning

        # Rate limiting.
        time.sleep(0.1)

    return trajectory


def main():
    parser = argparse.ArgumentParser(
        description="Add chain-of-thought reasoning to expert trajectories"
    )
    parser.add_argument(
        "--input-dir",
        default=str(PROJECT_ROOT / "data" / "expert_trajectories"),
    )
    parser.add_argument(
        "--output-dir",
        default=None,
        help="Output directory (default: overwrite input files)",
    )
    parser.add_argument(
        "--provider", default="google",
        choices=["google", "anthropic", "openai"],
    )
    parser.add_argument(
        "--model", default=None,
        help="Override model name",
    )
    parser.add_argument(
        "--cache-file",
        default=str(PROJECT_ROOT / "data" / "cot_cache.json"),
        help="Cache file for API responses",
    )
    args = parser.parse_args()

    with open(PROJECT_ROOT / "config" / "model_config.yaml") as f:
        model_config = yaml.safe_load(f)

    # Determine model.
    if args.model:
        model = args.model
    else:
        api_cfg = model_config["api_models"]
        if args.provider == "google":
            model = api_cfg.get("google", "gemini-3-flash-preview")
        elif args.provider == "anthropic":
            model = api_cfg["primary"]
        else:
            model = api_cfg["fallback"]

    client, model = _get_client(args.provider, model)

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir) if args.output_dir else input_dir

    if output_dir != input_dir:
        output_dir.mkdir(parents=True, exist_ok=True)

    # Load cache.
    cache_path = Path(args.cache_file)
    cache: dict[str, str] = {}
    if cache_path.exists():
        with open(cache_path) as f:
            cache = json.load(f)
        logger.info(f"Loaded {len(cache)} cached reasoning entries")

    # Process all trajectory files.
    files = sorted(input_dir.glob("step_*.json"))
    if not files:
        logger.error(f"No trajectory files found in {input_dir}")
        sys.exit(1)

    total_steps = 0
    total_annotated = 0

    for fpath in files:
        with open(fpath) as f:
            trajectories = json.load(f)

        for traj in trajectories:
            if not traj.get("success"):
                continue
            steps_before = sum(1 for s in traj.get("steps", []) if s.get("reasoning"))
            annotate_trajectory(traj, client, model, args.provider, cache)
            steps_after = sum(1 for s in traj.get("steps", []) if s.get("reasoning"))
            total_steps += len(traj.get("steps", []))
            total_annotated += steps_after - steps_before

        # Save annotated trajectories.
        out_path = output_dir / fpath.name
        with open(out_path, "w") as f:
            json.dump(trajectories, f, indent=2)
        logger.info(f"Annotated {fpath.name}")

    # Save cache.
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_path, "w") as f:
        json.dump(cache, f, indent=2)

    logger.info(f"\nAnnotation complete: {total_annotated} new reasoning entries")
    logger.info(f"Total steps processed: {total_steps}")
    logger.info(f"Cache size: {len(cache)} entries")


if __name__ == "__main__":
    main()
