#!/usr/bin/env python3
"""Add synthetic chain-of-thought reasoning to expert trajectories.

Takes expert trajectories (from collect_expert_trajectories.py) and annotates
each (observation, action) pair with 2-4 sentences of reasoning generated by
an LLM. The reasoning is inserted as the "reasoning" field in each step,
which sft_train.py already handles via format_as_conversations() to produce
<think>reasoning</think>action format.

Supports API providers (Google, Anthropic, OpenAI) and local inference via
vLLM for zero-cost annotation.

Usage:
    python -m src.training.generate_cot                                    # Gemini Flash (API)
    python -m src.training.generate_cot --provider local                   # Qwen3-14B-AWQ via vLLM (free)
    python -m src.training.generate_cot --provider local --model Qwen/Qwen3-8B-AWQ  # smaller/faster
    python -m src.training.generate_cot --provider anthropic --model claude-sonnet-4-20250514
"""

from __future__ import annotations

import argparse
import hashlib
import json
import logging
import sys
import time
from pathlib import Path

import yaml
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
load_dotenv(Path(__file__).resolve().parents[2] / ".env")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[2]

COT_PROMPT = """\
You are annotating expert browser agent trajectories with chain-of-thought reasoning.

Given the current page observation and the expert action taken, write 2-4 sentences
explaining WHY this action was chosen. Focus on:
1. What clues in the observation led to this action.
2. What the action is expected to accomplish.
3. Any puzzle-solving logic (e.g., "the code appears to be hidden in shadow DOM").

Be concise and specific. Do NOT repeat the observation or action verbatim.

## Current Observation (truncated):
{obs_text}

## Expert Action:
{action}

## Reasoning (2-4 sentences):"""


LOCAL_MODEL_DEFAULT = "Qwen/Qwen3-14B-AWQ"


def _get_client(provider: str, model: str):
    """Create an API client for the given provider."""
    if provider == "google":
        from google import genai
        return genai.Client(), model
    elif provider == "anthropic":
        import anthropic
        return anthropic.Anthropic(), model
    elif provider == "openai":
        import openai
        return openai.OpenAI(), model
    elif provider == "local":
        return None, model  # vLLM loaded separately in batch path.
    else:
        raise ValueError(f"Unknown provider: {provider}")


def _call_api(client, model: str, provider: str, prompt: str) -> str:
    """Make a single API call to generate reasoning."""
    if provider == "google":
        from google.genai import types

        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(max_output_tokens=1024),
        )
        return response.text.strip()
    elif provider == "anthropic":
        response = client.messages.create(
            model=model,
            max_tokens=256,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.content[0].text.strip()
    elif provider == "openai":
        response = client.chat.completions.create(
            model=model,
            max_tokens=256,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content.strip()
    return ""


def generate_reasoning(
    obs_text: str,
    action: str,
    client,
    model: str,
    provider: str,
    cache: dict[str, str],
) -> str:
    """Generate reasoning for a single (observation, action) pair.

    Uses a content-hash cache to avoid redundant API calls.
    """
    # Truncate observation to keep API cost low.
    obs_truncated = obs_text[:3000] if len(obs_text) > 3000 else obs_text

    cache_key = hashlib.md5(
        f"{obs_truncated}|{action}".encode()
    ).hexdigest()

    if cache_key in cache:
        logger.debug("    cache hit: %s", cache_key[:8])
        return cache[cache_key]

    prompt = COT_PROMPT.format(obs_text=obs_truncated, action=action)

    try:
        t0 = time.time()
        reasoning = _call_api(client, model, provider, prompt)
        logger.info(
            "    API call (%.1fs): action=%s → %d chars reasoning",
            time.time() - t0,
            action[:60],
            len(reasoning),
        )
    except Exception as e:
        logger.warning("    API call failed for action=%s: %s", action[:60], e)
        reasoning = ""

    cache[cache_key] = reasoning
    return reasoning


def annotate_trajectory(
    trajectory: dict,
    client,
    model: str,
    provider: str,
    cache: dict[str, str],
) -> dict:
    """Add reasoning field to each step in the trajectory."""
    for step in trajectory.get("steps", []):
        if step.get("reasoning"):
            continue  # Already annotated.

        reasoning = generate_reasoning(
            obs_text=step.get("obs_text", ""),
            action=step.get("action", ""),
            client=client,
            model=model,
            provider=provider,
            cache=cache,
        )
        step["reasoning"] = reasoning

        # Rate limiting.
        time.sleep(0.1)

    return trajectory


def _build_prompt(obs_text: str, action: str) -> str:
    """Build the CoT prompt for a single step, with truncation."""
    obs_truncated = obs_text[:3000] if len(obs_text) > 3000 else obs_text
    return COT_PROMPT.format(obs_text=obs_truncated, action=action)


def _cache_key(obs_text: str, action: str) -> str:
    """Compute the cache key for an (obs, action) pair."""
    obs_truncated = obs_text[:3000] if len(obs_text) > 3000 else obs_text
    return hashlib.md5(f"{obs_truncated}|{action}".encode()).hexdigest()


def run_local_batch(
    files: list[Path],
    output_dir: Path,
    model_name: str,
    cache: dict[str, str],
    cache_path: Path,
) -> tuple[int, int]:
    """Generate all CoT annotations via vLLM batched inference.

    Collects all unannotated steps, builds prompts, runs them through vLLM
    in one batch, then writes results back to trajectory files.

    Returns (total_steps, newly_annotated).
    """
    from vllm import LLM, SamplingParams

    # --- Phase 1: Collect all unannotated steps ---
    logger.info("Scanning for unannotated steps...")
    # Each item: (file_idx, traj_idx, step_idx, cache_key, prompt)
    work_items: list[tuple[int, int, int, str, str]] = []
    # Parallel structure: loaded trajectory data per file.
    all_file_data: list[tuple[Path, list[dict]]] = []

    for fpath in files:
        with open(fpath) as f:
            trajectories = json.load(f)
        all_file_data.append((fpath, trajectories))

        file_idx = len(all_file_data) - 1
        for traj_idx, traj in enumerate(trajectories):
            if not traj.get("success"):
                continue
            for step_idx, step in enumerate(traj.get("steps", [])):
                if step.get("reasoning"):
                    continue  # Already annotated.
                key = _cache_key(
                    step.get("obs_text", ""), step.get("action", ""),
                )
                if key in cache:
                    # Apply cached result immediately.
                    step["reasoning"] = cache[key]
                    continue
                prompt = _build_prompt(
                    step.get("obs_text", ""), step.get("action", ""),
                )
                work_items.append((file_idx, traj_idx, step_idx, key, prompt))

    logger.info(
        "Found %d steps needing generation (%d served from cache)",
        len(work_items),
        sum(
            1 for _, trajs in all_file_data
            for t in trajs if t.get("success")
            for s in t.get("steps", []) if s.get("reasoning")
        ),
    )

    if not work_items:
        logger.info("All steps already annotated, nothing to do.")
        # Still save files (cache hits may have been applied).
        for fpath, trajectories in all_file_data:
            out_path = output_dir / fpath.name
            with open(out_path, "w") as f:
                json.dump(trajectories, f, indent=2)
        return 0, 0

    # --- Phase 2: Load model and generate ---
    logger.info("Loading vLLM model: %s", model_name)
    t0 = time.time()
    llm = LLM(
        model=model_name,
        gpu_memory_utilization=0.85,
        max_model_len=4096,
        enforce_eager=True,  # Avoid CUDA graph overhead for single batch.
    )
    logger.info("Model loaded in %.1fs", time.time() - t0)

    # Format prompts using Qwen3 chat template with thinking disabled.
    # enable_thinking=False prepends an empty <think></think> block,
    # suppressing the model's internal reasoning and producing concise output.
    tokenizer = llm.get_tokenizer()
    raw_prompts = [item[4] for item in work_items]
    formatted_prompts = []
    for raw_prompt in raw_prompts:
        messages = [{"role": "user", "content": raw_prompt}]
        formatted = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,
        )
        formatted_prompts.append(formatted)
    logger.info(
        "Formatted %d prompts with chat template (thinking disabled)",
        len(formatted_prompts),
    )

    sampling_params = SamplingParams(
        temperature=0.3,  # Low temperature for consistent annotations.
        max_tokens=256,
        stop=["\n\n\n", "##"],  # Stop after reasoning block.
    )

    logger.info("Generating %d annotations in one batch...", len(formatted_prompts))
    t0 = time.time()
    outputs = llm.generate(formatted_prompts, sampling_params)
    gen_time = time.time() - t0
    logger.info(
        "Generation complete: %d outputs in %.1fs (%.1f steps/sec)",
        len(outputs), gen_time, len(outputs) / gen_time,
    )

    # --- Phase 3: Map results back ---
    newly_annotated = 0
    for item, output in zip(work_items, outputs):
        file_idx, traj_idx, step_idx, key, _ = item
        reasoning = output.outputs[0].text.strip()

        _, trajectories = all_file_data[file_idx]
        trajectories[traj_idx]["steps"][step_idx]["reasoning"] = reasoning
        cache[key] = reasoning
        newly_annotated += 1

    # --- Phase 4: Save everything ---
    logger.info("Saving annotated files and cache...")
    for fpath, trajectories in all_file_data:
        out_path = output_dir / fpath.name
        with open(out_path, "w") as f:
            json.dump(trajectories, f, indent=2)

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_path, "w") as f:
        json.dump(cache, f, indent=2)

    total_steps = sum(
        len(t.get("steps", []))
        for _, trajs in all_file_data
        for t in trajs if t.get("success")
    )
    logger.info(
        "Done: %d newly annotated, %d total steps, cache: %d entries",
        newly_annotated, total_steps, len(cache),
    )
    return total_steps, newly_annotated


def main():
    parser = argparse.ArgumentParser(
        description="Add chain-of-thought reasoning to expert trajectories"
    )
    parser.add_argument(
        "--input-dir",
        default=str(PROJECT_ROOT / "data" / "expert_trajectories"),
    )
    parser.add_argument(
        "--output-dir",
        default=None,
        help="Output directory (default: overwrite input files)",
    )
    parser.add_argument(
        "--provider", default="google",
        choices=["google", "anthropic", "openai", "local"],
    )
    parser.add_argument(
        "--model", default=None,
        help="Override model name",
    )
    parser.add_argument(
        "--cache-file",
        default=str(PROJECT_ROOT / "data" / "cot_cache.json"),
        help="Cache file for API responses",
    )
    args = parser.parse_args()

    with open(PROJECT_ROOT / "config" / "model_config.yaml") as f:
        model_config = yaml.safe_load(f)

    # Determine model.
    if args.model:
        model = args.model
    elif args.provider == "local":
        model = LOCAL_MODEL_DEFAULT
    else:
        api_cfg = model_config["api_models"]
        if args.provider == "google":
            model = api_cfg.get("google", "gemini-3-flash-preview")
        elif args.provider == "anthropic":
            model = api_cfg["primary"]
        else:
            model = api_cfg["fallback"]

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir) if args.output_dir else input_dir

    if output_dir != input_dir:
        output_dir.mkdir(parents=True, exist_ok=True)

    # Load cache.
    cache_path = Path(args.cache_file)
    cache: dict[str, str] = {}
    if cache_path.exists():
        with open(cache_path) as f:
            cache = json.load(f)
        logger.info(f"Loaded {len(cache)} cached reasoning entries")

    # Process all trajectory files.
    files = sorted(input_dir.glob("step_*.json"))
    if not files:
        logger.error(f"No trajectory files found in {input_dir}")
        sys.exit(1)

    logger.info("Provider: %s, Model: %s", args.provider, model)

    # --- Local provider: vLLM batched inference (all at once) ---
    if args.provider == "local":
        run_local_batch(files, output_dir, model, cache, cache_path)
        return

    # --- API providers: sequential per-trajectory ---
    client, model = _get_client(args.provider, model)

    # Pre-scan to get totals for progress reporting.
    total_steps = 0
    steps_needing_cot = 0
    for fpath in files:
        with open(fpath) as f:
            trajs = json.load(f)
        for traj in trajs:
            if not traj.get("success"):
                continue
            for s in traj.get("steps", []):
                total_steps += 1
                if not s.get("reasoning"):
                    steps_needing_cot += 1

    logger.info(
        "Found %d files, %d total steps across successful trajectories, "
        "%d need CoT annotation",
        len(files), total_steps, steps_needing_cot,
    )

    total_steps = 0
    total_annotated = 0
    run_start = time.time()
    cache_path.parent.mkdir(parents=True, exist_ok=True)

    for file_idx, fpath in enumerate(files, 1):
        with open(fpath) as f:
            trajectories = json.load(f)

        successful = [t for t in trajectories if t.get("success")]
        if not successful:
            logger.info(
                "[%d/%d] %s — no successful trajectories, skipping",
                file_idx, len(files), fpath.name,
            )
            continue

        file_steps = sum(len(t.get("steps", [])) for t in successful)
        already_annotated = sum(
            1 for t in successful for s in t.get("steps", []) if s.get("reasoning")
        )
        logger.info(
            "[%d/%d] %s — %d trajectories, %d steps (%d already annotated)",
            file_idx, len(files), fpath.name,
            len(successful), file_steps, already_annotated,
        )

        file_dirty = False
        for traj_idx, traj in enumerate(trajectories):
            if not traj.get("success"):
                continue

            steps_before = sum(1 for s in traj.get("steps", []) if s.get("reasoning"))
            annotate_trajectory(traj, client, model, args.provider, cache)
            steps_after = sum(1 for s in traj.get("steps", []) if s.get("reasoning"))

            newly_annotated = steps_after - steps_before
            total_steps += len(traj.get("steps", []))
            total_annotated += newly_annotated

            logger.info(
                "  traj %d (step %s): %d steps, +%d reasoning (%.0fs elapsed)",
                traj_idx + 1,
                traj.get("step_number", "?"),
                len(traj.get("steps", [])),
                newly_annotated,
                time.time() - run_start,
            )

            # Save after every trajectory so progress survives Ctrl-C.
            if newly_annotated > 0:
                file_dirty = True
                out_path = output_dir / fpath.name
                with open(out_path, "w") as f:
                    json.dump(trajectories, f, indent=2)
                with open(cache_path, "w") as f:
                    json.dump(cache, f, indent=2)

        if file_dirty:
            logger.info(
                "  Saved %s (cache: %d entries)",
                fpath.name, len(cache),
            )

    elapsed = time.time() - run_start
    logger.info("=" * 60)
    logger.info("Annotation complete in %.1fs", elapsed)
    logger.info(
        "  %d new reasoning entries added, %d total steps processed",
        total_annotated, total_steps,
    )
    logger.info("  Cache: %d entries saved to %s", len(cache), cache_path)


if __name__ == "__main__":
    main()
